{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "elementary-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy import stats\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "indie-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensory_words_traindf = pd.DataFrame()\n",
    "sensory_words_testdf = pd.DataFrame()\n",
    "words_embedding_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "weekly-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assigned_words(seq_clusters, cluster_words, axis, flag_train=False):\n",
    "\n",
    "    # _assign word to each cluster of the subsequence usnig numpy where function\n",
    "    assigned_words = np.where(\n",
    "        seq_clusters != 0, seq_clusters, cluster_words[0])\n",
    "    for idx in range(1, len(cluster_words)):\n",
    "        assigned_words = np.where(\n",
    "            seq_clusters != idx, assigned_words, cluster_words[idx])\n",
    "\n",
    "    if flag_train:\n",
    "        sensory_words_traindf[axis] = assigned_words\n",
    "        assigned_clusterWord = pd.DataFrame(\n",
    "            data=assigned_words, columns=['cluster_word'])\n",
    "        return assigned_clusterWord\n",
    "    else:\n",
    "        sensory_words_testdf[axis] = assigned_words\n",
    "\n",
    "\n",
    "def clustering(statistic_train_df, statistic_test_df, axis, cluster_cnts, cluster_words):\n",
    "\n",
    "    statistic_train_df = Normalizer().fit_transform(np.array(statistic_train_df))\n",
    "    statistic_test_df = Normalizer().fit_transform(np.array(statistic_test_df))\n",
    "\n",
    "    model = KMeans(n_clusters=cluster_cnts,\n",
    "                   random_state=234).fit(statistic_train_df)\n",
    "\n",
    "    cluster_ids = pd.DataFrame(model.predict(\n",
    "        statistic_train_df), columns=['cluster ID'])\n",
    "    cluster_test_ids = pd.DataFrame(model.predict(\n",
    "        statistic_test_df), columns=['cluster ID'])\n",
    "\n",
    "    seq_clusters = cluster_ids.to_numpy()\n",
    "    assigned_clusterWord = get_assigned_words(\n",
    "        seq_clusters, cluster_words, axis, flag_train=True)\n",
    "    get_assigned_words(cluster_test_ids.to_numpy(), cluster_words, axis)\n",
    "\n",
    "    centroids_of_clusters = pd.DataFrame(model.cluster_centers_[cluster_ids['cluster ID']],\n",
    "                                         columns=[f'dim_{val}' for val in range(statistic_train_df.shape[1])])\n",
    "    result = pd.concat([assigned_clusterWord, centroids_of_clusters], axis=1)\n",
    "    result = result.drop_duplicates()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# generating names for cluster count\n",
    "def generate_cluster_names(sequence_names, cluster_cnt=100):\n",
    "\n",
    "    words_dict = {}\n",
    "\n",
    "    for seq in sequence_names:\n",
    "        prefix = seq\n",
    "        words_dict[seq] = [prefix+'_'+str(i) for i in range(cluster_cnt)]\n",
    "\n",
    "    return words_dict\n",
    "\n",
    "\n",
    "def cluster_word_sort(axis_clusters, cluster_names):\n",
    "\n",
    "    result = axis_clusters.loc[(\n",
    "        axis_clusters['cluster_word'] == cluster_names)]\n",
    "\n",
    "    return result.iloc[:, 1:]\n",
    "\n",
    "\n",
    "def perform_clustering(statistics_train, statistics_test, channels, cluster_cnts, words_generation_flag=False):\n",
    "\n",
    "    centroid_statistic = []\n",
    "\n",
    "    words_dict = generate_cluster_names(channels, cluster_cnts)\n",
    "\n",
    "    for statistic_train_df, statistic_test_df, axis in zip(statistics_train, statistics_test, channels):\n",
    "\n",
    "        cluster_names = words_dict[axis]\n",
    "        axis_clusters = clustering(\n",
    "            statistic_train_df, statistic_test_df, axis, cluster_cnts, cluster_names)\n",
    "\n",
    "        for j in range(len(cluster_names)):\n",
    "\n",
    "            cluster_stats = cluster_word_sort(axis_clusters, cluster_names[j])\n",
    "            centroid_statistic.append(cluster_stats)\n",
    "            words_embedding_dict[cluster_names[j]] = cluster_stats.values[0].tolist()\n",
    "\n",
    "    # stop words generation\n",
    "    stop_words_generation(channels)\n",
    "    if words_generation_flag:\n",
    "        # new words generations inter sensor channels for train\n",
    "        new_words_generation(channels, flag_train=True)\n",
    "        # new words generations inter sensor channels for test\n",
    "        new_words_generation(channels)\n",
    "        embeddings_filepath = os.getcwd(\n",
    "        ) + f'/../data/sub_sequence_output/word_embeddings_from_clusters.json'\n",
    "        with open(embeddings_filepath, 'w') as fp:\n",
    "            json.dump(words_embedding_dict, fp)\n",
    "    else:\n",
    "\n",
    "        embeddings_filepath = os.getcwd(\n",
    "        ) + f'/../data/sub_sequence_output/word_embeddings_from_clusters.txt'\n",
    "        pd.concat(centroid_statistic).to_csv(\n",
    "            embeddings_filepath, index=False, header=False)\n",
    "\n",
    "    # writing train documents to text files\n",
    "    write_clustering_output(sensory_words_traindf.columns[2:], flag_train=True)\n",
    "    # writing test documents to text files\n",
    "    write_clustering_output(sensory_words_testdf.columns[2:])\n",
    "\n",
    "    print(f'Finished generate_subsequences_uci_har  : {cluster_cnts} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "induced-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words_generation(channels):\n",
    "\n",
    "    stop_word_list = []\n",
    "\n",
    "    def each_channel(channel):\n",
    "\n",
    "        stopwords = sensory_words_traindf[[channel, 'activityID']].groupby(channel)[\n",
    "            'activityID'].nunique()\n",
    "        stopwords = stopwords[stopwords > 3].keys().tolist()\n",
    "\n",
    "        return stopwords\n",
    "\n",
    "    for channel in channels:\n",
    "\n",
    "        stop_word_list.extend(each_channel(channel))\n",
    "\n",
    "    with open(os.getcwd() + f'/../data/stopwords.pkl', 'wb') as f:\n",
    "        pickle.dump(stop_word_list, f)\n",
    "\n",
    "\n",
    "def write_clustering_output(channels, flag_train=False):\n",
    "\n",
    "    if flag_train:\n",
    "\n",
    "        # _combine individual words as documents\n",
    "        sensory_words_traindf['final_sub_sequence'] = sensory_words_traindf[channels].apply(\n",
    "            lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        # _save the combined values to text files\n",
    "        for subject in sensory_words_traindf['subject_id'].unique():\n",
    "            activity = sensory_words_traindf.loc[(\n",
    "                sensory_words_traindf['subject_id'] == subject)]['activityID'].values[0]\n",
    "            output_filepath = os.getcwd() + f'/../data/documents/train/activity_subseq_' + \\\n",
    "                str(subject) + '_' + str(activity) + '.txt'\n",
    "            sensory_words_traindf.loc[(sensory_words_traindf['subject_id'] == subject)][[\n",
    "                'final_sub_sequence']].to_csv(output_filepath, sep='\\t', index=False, header=False)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # _combine individual words as documents\n",
    "        sensory_words_testdf['final_sub_sequence'] = sensory_words_testdf[channels].apply(\n",
    "            lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        # _save the combined values to text files\n",
    "        for subject in sensory_words_testdf['subject_id'].unique():\n",
    "            activity = sensory_words_testdf.loc[(\n",
    "                sensory_words_testdf['subject_id'] == subject)]['activityID'].values[0]\n",
    "            output_filepath = os.getcwd() + f'/../data/documents/test/activity_subseq_' + \\\n",
    "                str(subject) + '_' + str(activity) + '.txt'\n",
    "            sensory_words_testdf.loc[(sensory_words_testdf['subject_id'] == subject)][[\n",
    "                'final_sub_sequence']].to_csv(output_filepath, sep='\\t', index=False, header=False)\n",
    "\n",
    "\n",
    "def form_words4(row, flag_train=False):\n",
    "\n",
    "    temp = []\n",
    "    temp = row.values\n",
    "    if flag_train:\n",
    "        words_embedding_dict[temp[0]+temp[1]+temp[2]+temp[3]] = [v1+v2+v3+v4\n",
    "                                                                 for v1, v2, v3, v4 in zip(words_embedding_dict[temp[0]], words_embedding_dict[temp[1]], words_embedding_dict[temp[2]], words_embedding_dict[temp[2]])]\n",
    "\n",
    "    return ''.join(temp.astype(str))\n",
    "\n",
    "\n",
    "def form_words3(row, flag_train=False):\n",
    "\n",
    "    temp = []\n",
    "    temp = row.values\n",
    "    if flag_train:\n",
    "        words_embedding_dict[temp[0]+temp[1]+temp[2]] =  [x + y + z\n",
    "        for x, y, z in zip(words_embedding_dict[temp[0]], words_embedding_dict[temp[1]], words_embedding_dict[temp[2]])]\n",
    "\n",
    "    return ''.join(temp.astype(str))\n",
    "\n",
    "def new_words_generation(channels, flag_train=False):\n",
    "\n",
    "    word_combinations = [['X1', 'Y1', 'Y2', 'Z2'], ['X1', 'Z1', 'X2', 'Y2'], ['Y1', 'Z1', 'X2', 'Z2'], [\n",
    "        'X1', 'Y1', 'X2', 'Z2'], ['X1', 'Z1', 'Y2', 'Z2'], ['Y1', 'Z1', 'X2', 'Y2']]\n",
    "\n",
    "    # _adding four word combinations\n",
    "    for idx, combinations in enumerate(word_combinations):\n",
    "        acc_axis = combinations[0]\n",
    "        temp = combinations[1:]\n",
    "        if flag_train:\n",
    "            sensory_words_traindf[acc_axis + temp[0] + temp[1] + temp[2]] = sensory_words_traindf[[\n",
    "                acc_axis, temp[0], temp[1], temp[2]]].apply(lambda row: form_words4(row, flag_train), axis=1)\n",
    "        else:\n",
    "            sensory_words_testdf[acc_axis + temp[0] + temp[1] + temp[2]] = sensory_words_testdf[[\n",
    "                acc_axis, temp[0], temp[1], temp[2]]].apply(lambda row: form_words4(row), axis=1)\n",
    "\n",
    "    # _adding three word combinations\n",
    "    # for acc_axis in channels[:3]:\n",
    "    #     temp = []\n",
    "    #     for gyro_axis in channels[3:]:\n",
    "\n",
    "    #         if acc_axis[0] != gyro_axis[0]:\n",
    "\n",
    "    #             temp.append(gyro_axis)\n",
    "    #     if flag_train:\n",
    "    #         sensory_words_traindf[acc_axis + temp[0] + temp[1]] = sensory_words_traindf[[acc_axis, temp[0], temp[1]]].apply(lambda row: form_words3(row, flag_train), axis=1)\n",
    "    #     else:\n",
    "    #         sensory_words_testdf[acc_axis + temp[0] + temp[1]] = sensory_words_testdf[[acc_axis, temp[0], temp[1]]].apply(lambda row: form_words3(row), axis=1)\n",
    "\n",
    "\n",
    "def load_train_test_data(input_file_path, col_names):\n",
    "\n",
    "    main_df = pd.read_csv(input_file_path, names=col_names)\n",
    "    main_df = main_df.astype({'subject_id': int, 'activityID': int})\n",
    "\n",
    "    return main_df\n",
    "\n",
    "\n",
    "def window_sampling(main_df, window_length, window_overlap):\n",
    "\n",
    "    max_window_index = len(main_df.index)\n",
    "    num_of_subsequences = len(col_names)\n",
    "    sub_sequences = [[] for x in range(num_of_subsequences)]\n",
    "\n",
    "    window_index = 0\n",
    "\n",
    "    while window_index <= (max_window_index - window_length):\n",
    "\n",
    "        activity_sequence = main_df[col_names[1]\n",
    "                                    ][window_index:window_index+window_length].tolist()\n",
    "        subject_sequence = main_df[col_names[0]\n",
    "                                   ][window_index:window_index+window_length].tolist()\n",
    "\n",
    "        if len(set(subject_sequence)) == 1:\n",
    "            sub_sequences[1].append(activity_sequence[0])\n",
    "            sub_sequences[0].append(subject_sequence[0])\n",
    "\n",
    "            for idx in range(2, num_of_subsequences):\n",
    "                sub_sequences[idx].append(\n",
    "                    main_df[col_names[idx]][window_index:window_index+window_length].tolist())\n",
    "\n",
    "        window_index += window_overlap\n",
    "\n",
    "    # _converting into numpy arrays\n",
    "    np_sequences = np.asarray(sub_sequences[2:])\n",
    "    subject_activity_seq = np.asarray(sub_sequences[:2])\n",
    "\n",
    "    return subject_activity_seq, np_sequences\n",
    "\n",
    "\n",
    "def feature_sum(vec_list):\n",
    "    vec_list = np.array(vec_list)\n",
    "    vec_sum = vec_list[0]\n",
    "\n",
    "    for idx in range(1, len(vec_list)):\n",
    "        vec_sum += vec_list[idx]\n",
    "\n",
    "    return vec_sum.tolist()\n",
    "\n",
    "\n",
    "def onehotenocding_pooling(subject_activity_seq, subsequences, pooling_size=3):\n",
    "\n",
    "    max_window_index = subject_activity_seq.shape[1]\n",
    "    num_of_subsequences = len(col_names)\n",
    "    pooled_features = [[] for x in range(num_of_subsequences)]\n",
    "    window_index = 0\n",
    "\n",
    "    while window_index <= (max_window_index - pooling_size):\n",
    "\n",
    "        activity_sequence = subject_activity_seq[1\n",
    "                                                 ][window_index:window_index+pooling_size].tolist()\n",
    "        subject_sequence = subject_activity_seq[0\n",
    "                                                ][window_index:window_index+pooling_size].tolist()\n",
    "\n",
    "        if len(set(subject_sequence)) == 1:\n",
    "            pooled_features[1].append(activity_sequence[0])\n",
    "            pooled_features[0].append(subject_sequence[0])\n",
    "\n",
    "            for idx in range(2, num_of_subsequences):\n",
    "                pooled_features[idx].append(\n",
    "                    feature_sum(subsequences[idx-2][window_index:window_index+pooling_size]))\n",
    "\n",
    "        window_index += pooling_size\n",
    "\n",
    "    # _converting into numpy arrays\n",
    "    np_sequences = np.asarray(pooled_features[2:])\n",
    "    subject_activity_seq = np.asarray(pooled_features[:2])\n",
    "\n",
    "    return subject_activity_seq, np_sequences\n",
    "\n",
    "\n",
    "def get_kmeans_clusters(sub_sequence_train, sub_sequence_test, feature_dim):\n",
    "\n",
    "    model = KMeans(n_clusters=feature_dim,\n",
    "                   random_state=5).fit(sub_sequence_train)\n",
    "    cluster_train_ids = pd.DataFrame(model.predict(\n",
    "        sub_sequence_train), columns=['cluster ID'])\n",
    "    cluster_test_ids = pd.DataFrame(model.predict(\n",
    "        sub_sequence_test), columns=['cluster ID'])\n",
    "\n",
    "    one_hot_features_train = pd.get_dummies(cluster_train_ids.astype('str'))\n",
    "    one_hot_features_test = pd.get_dummies(cluster_test_ids.astype('str'))\n",
    "\n",
    "    return one_hot_features_train.values.tolist(), one_hot_features_test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "joined-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting windowing features....\n",
      "Starting feature clustering and pooling ....\n",
      "Finished feature extraction  : 8, 16, 16 \n",
      "Starting Clustering  : 8, 16, 16 \n",
      "Finished generate_subsequences_uci_har  : 8 \n",
      "Ending histogram features....\n"
     ]
    }
   ],
   "source": [
    "feature_dim = 5\n",
    "pooling_size = 2\n",
    "cluster_cnts = 8\n",
    "window_length = 16\n",
    "window_overlap = 16\n",
    "\n",
    "train_file_path = os.getcwd() + f'/../data/output_csv/processed_data_train.csv'\n",
    "test_file_path = os.getcwd() + f'/../data/output_csv/processed_data_test.csv'\n",
    "col_names = ['subject_id', 'activityID',\n",
    "             'X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']\n",
    "\n",
    "train_df = load_train_test_data(train_file_path, col_names)\n",
    "test_df = load_train_test_data(test_file_path, col_names)\n",
    "\n",
    "print('Starting windowing features....')\n",
    "\n",
    "train_subject_activity_seq, train_subsequences = window_sampling(\n",
    "    train_df, window_length=window_length, window_overlap=window_overlap)\n",
    "test_subject_activity_seq, test_subsequences = window_sampling(\n",
    "    test_df, window_length=window_length, window_overlap=window_overlap)\n",
    "\n",
    "assert train_subject_activity_seq.shape[1] == train_subsequences.shape[1]\n",
    "assert test_subject_activity_seq.shape[1] == test_subsequences.shape[1]\n",
    "\n",
    "pooled_features_train = []\n",
    "pooled_features_test = []\n",
    "\n",
    "print('Starting feature clustering and pooling ....')\n",
    "for idx in range(6):\n",
    "    train_data, test_data = get_kmeans_clusters(\n",
    "        train_subsequences[idx], test_subsequences[idx], feature_dim)\n",
    "\n",
    "    pooled_features_train.append(train_data)\n",
    "    pooled_features_test.append(test_data)\n",
    "\n",
    "pooled_features_train = np.array(pooled_features_train)\n",
    "pooled_features_test = np.array(pooled_features_test)\n",
    "\n",
    "train_subject_activity_seq_pooled, train_subsequences_pooled = onehotenocding_pooling(\n",
    "    train_subject_activity_seq, pooled_features_train, pooling_size)\n",
    "test_subject_activity_seq_pooled, test_subsequences_pooled = onehotenocding_pooling(\n",
    "    test_subject_activity_seq, pooled_features_test, pooling_size)\n",
    "\n",
    "assert train_subject_activity_seq_pooled.shape[1] == train_subsequences_pooled.shape[1]\n",
    "assert test_subject_activity_seq_pooled.shape[1] == test_subsequences_pooled.shape[1]\n",
    "\n",
    "sensory_words_traindf['subject_id'] = train_subject_activity_seq_pooled[0]\n",
    "sensory_words_traindf['activityID'] = train_subject_activity_seq_pooled[1]\n",
    "\n",
    "sensory_words_testdf['subject_id'] = test_subject_activity_seq_pooled[0]\n",
    "sensory_words_testdf['activityID'] = test_subject_activity_seq_pooled[1]\n",
    "\n",
    "print(\n",
    "    f'Finished feature extraction  : {cluster_cnts}, {window_length}, {window_overlap} ')\n",
    "\n",
    "print(\n",
    "    f'Starting Clustering  : {cluster_cnts}, {window_length}, {window_overlap} ')\n",
    "\n",
    "perform_clustering(train_subsequences_pooled, test_subsequences_pooled,\n",
    "                   channels=col_names[2:], cluster_cnts=cluster_cnts, words_generation_flag=True)\n",
    "\n",
    "print('Ending histogram features....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "natural-anime",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>activityID</th>\n",
       "      <th>X1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Z1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Z2</th>\n",
       "      <th>X1Y1Y2Z2</th>\n",
       "      <th>X1Z1X2Y2</th>\n",
       "      <th>Y1Z1X2Z2</th>\n",
       "      <th>X1Y1X2Z2</th>\n",
       "      <th>X1Z1Y2Z2</th>\n",
       "      <th>Y1Z1X2Y2</th>\n",
       "      <th>final_sub_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>X1_0</td>\n",
       "      <td>Y1_4</td>\n",
       "      <td>Z1_5</td>\n",
       "      <td>X2_0</td>\n",
       "      <td>Y2_0</td>\n",
       "      <td>Z2_0</td>\n",
       "      <td>X1_0Y1_4Y2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5X2_0Y2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Z2_0</td>\n",
       "      <td>X1_0Y1_4X2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5Y2_0Z2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Y2_0</td>\n",
       "      <td>X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>X1_0</td>\n",
       "      <td>Y1_4</td>\n",
       "      <td>Z1_5</td>\n",
       "      <td>X2_0</td>\n",
       "      <td>Y2_0</td>\n",
       "      <td>Z2_0</td>\n",
       "      <td>X1_0Y1_4Y2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5X2_0Y2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Z2_0</td>\n",
       "      <td>X1_0Y1_4X2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5Y2_0Z2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Y2_0</td>\n",
       "      <td>X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>X1_0</td>\n",
       "      <td>Y1_4</td>\n",
       "      <td>Z1_5</td>\n",
       "      <td>X2_0</td>\n",
       "      <td>Y2_0</td>\n",
       "      <td>Z2_0</td>\n",
       "      <td>X1_0Y1_4Y2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5X2_0Y2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Z2_0</td>\n",
       "      <td>X1_0Y1_4X2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5Y2_0Z2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Y2_0</td>\n",
       "      <td>X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>X1_0</td>\n",
       "      <td>Y1_4</td>\n",
       "      <td>Z1_5</td>\n",
       "      <td>X2_0</td>\n",
       "      <td>Y2_0</td>\n",
       "      <td>Z2_0</td>\n",
       "      <td>X1_0Y1_4Y2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5X2_0Y2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Z2_0</td>\n",
       "      <td>X1_0Y1_4X2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5Y2_0Z2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Y2_0</td>\n",
       "      <td>X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>X1_0</td>\n",
       "      <td>Y1_4</td>\n",
       "      <td>Z1_5</td>\n",
       "      <td>X2_0</td>\n",
       "      <td>Y2_0</td>\n",
       "      <td>Z2_0</td>\n",
       "      <td>X1_0Y1_4Y2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5X2_0Y2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Z2_0</td>\n",
       "      <td>X1_0Y1_4X2_0Z2_0</td>\n",
       "      <td>X1_0Z1_5Y2_0Z2_0</td>\n",
       "      <td>Y1_4Z1_5X2_0Y2_0</td>\n",
       "      <td>X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  activityID    X1    Y1    Z1    X2    Y2    Z2  \\\n",
       "0           1           5  X1_0  Y1_4  Z1_5  X2_0  Y2_0  Z2_0   \n",
       "1           1           5  X1_0  Y1_4  Z1_5  X2_0  Y2_0  Z2_0   \n",
       "2           1           5  X1_0  Y1_4  Z1_5  X2_0  Y2_0  Z2_0   \n",
       "3           1           5  X1_0  Y1_4  Z1_5  X2_0  Y2_0  Z2_0   \n",
       "4           2           5  X1_0  Y1_4  Z1_5  X2_0  Y2_0  Z2_0   \n",
       "\n",
       "           X1Y1Y2Z2          X1Z1X2Y2          Y1Z1X2Z2          X1Y1X2Z2  \\\n",
       "0  X1_0Y1_4Y2_0Z2_0  X1_0Z1_5X2_0Y2_0  Y1_4Z1_5X2_0Z2_0  X1_0Y1_4X2_0Z2_0   \n",
       "1  X1_0Y1_4Y2_0Z2_0  X1_0Z1_5X2_0Y2_0  Y1_4Z1_5X2_0Z2_0  X1_0Y1_4X2_0Z2_0   \n",
       "2  X1_0Y1_4Y2_0Z2_0  X1_0Z1_5X2_0Y2_0  Y1_4Z1_5X2_0Z2_0  X1_0Y1_4X2_0Z2_0   \n",
       "3  X1_0Y1_4Y2_0Z2_0  X1_0Z1_5X2_0Y2_0  Y1_4Z1_5X2_0Z2_0  X1_0Y1_4X2_0Z2_0   \n",
       "4  X1_0Y1_4Y2_0Z2_0  X1_0Z1_5X2_0Y2_0  Y1_4Z1_5X2_0Z2_0  X1_0Y1_4X2_0Z2_0   \n",
       "\n",
       "           X1Z1Y2Z2          Y1Z1X2Y2  \\\n",
       "0  X1_0Z1_5Y2_0Z2_0  Y1_4Z1_5X2_0Y2_0   \n",
       "1  X1_0Z1_5Y2_0Z2_0  Y1_4Z1_5X2_0Y2_0   \n",
       "2  X1_0Z1_5Y2_0Z2_0  Y1_4Z1_5X2_0Y2_0   \n",
       "3  X1_0Z1_5Y2_0Z2_0  Y1_4Z1_5X2_0Y2_0   \n",
       "4  X1_0Z1_5Y2_0Z2_0  Y1_4Z1_5X2_0Y2_0   \n",
       "\n",
       "                                  final_sub_sequence  \n",
       "0  X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...  \n",
       "1  X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...  \n",
       "2  X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...  \n",
       "3  X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...  \n",
       "4  X1_0 Y1_4 Z1_5 X2_0 Y2_0 Z2_0 X1_0Y1_4Y2_0Z2_0...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "altered-rogers",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praveen/.local/share/virtualenvs/Human_Activity_Recognition-8gA5e_Fl/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Z1</th>\n",
       "      <th>X2</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Z2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activityID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            X1  Y1  Z1  X2  Y2  Z2\n",
       "activityID                        \n",
       "1            7   6   7   8   8   8\n",
       "2            7   7   7   8   8   8\n",
       "3            7   6   7   8   8   8\n",
       "4            3   5   8   7   8   8\n",
       "5            3   5   7   8   7   8\n",
       "6            1   7   8   8   6   6"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf[['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2', 'activityID']].groupby('activityID')['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "motivated-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensory_words_traindf_1 = sensory_words_traindf.loc[(sensory_words_traindf['activityID'] == 1)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_traindf_2 = sensory_words_traindf.loc[(sensory_words_traindf['activityID'] == 2)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_traindf_3 = sensory_words_traindf.loc[(sensory_words_traindf['activityID'] == 3)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_traindf_4 = sensory_words_traindf.loc[(sensory_words_traindf['activityID'] == 4)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_traindf_5 = sensory_words_traindf.loc[(sensory_words_traindf['activityID'] == 5)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_traindf_6 = sensory_words_traindf.loc[(sensory_words_traindf['activityID'] == 6)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "golden-gazette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    Y1    Z1    X2    Y2    Z2  \n",
       "X1_5  Y1_5  Z1_4  X2_4  Y2_0  Z2_5    14\n",
       "      Y1_4  Z1_5  X2_4  Y2_0  Z2_1    14\n",
       "            Z1_4  X2_3  Y2_6  Z2_2    14\n",
       "            Z1_5  X2_3  Y2_0  Z2_2    13\n",
       "X1_2  Y1_1  Z1_5  X2_6  Y2_2  Z2_3    13\n",
       "                                      ..\n",
       "X1_3  Y1_4  Z1_2  X2_6  Y2_7  Z2_4     1\n",
       "X1_4  Y1_5  Z1_1  X2_3  Y2_6  Z2_5     1\n",
       "                  X2_6  Y2_2  Z2_0     1\n",
       "X1_3  Y1_4  Z1_1  X2_7  Y2_1  Z2_4     1\n",
       "X1_5  Y1_4  Z1_1  X2_6  Y2_2  Z2_7     1\n",
       "Length: 2028, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "nervous-guess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    Y1    Z1    X2    Y2    Z2  \n",
       "X1_3  Y1_1  Z1_1  X2_5  Y2_6  Z2_2    13\n",
       "X1_4  Y1_5  Z1_4  X2_2  Y2_7  Z2_1    11\n",
       "            Z1_6  X2_0  Y2_2  Z2_3     8\n",
       "X1_5  Y1_5  Z1_1  X2_7  Y2_1  Z2_0     8\n",
       "      Y1_1  Z1_3  X2_4  Y2_7  Z2_1     8\n",
       "                                      ..\n",
       "X1_3  Y1_5  Z1_2  X2_7  Y2_2  Z2_6     1\n",
       "X1_5  Y1_1  Z1_5  X2_2  Y2_5  Z2_3     1\n",
       "X1_2  Y1_1  Z1_5  X2_4  Y2_1  Z2_0     1\n",
       "X1_5  Y1_5  Z1_4  X2_3  Y2_5  Z2_4     1\n",
       "      Y1_1  Z1_5  X2_0  Y2_1  Z2_4     1\n",
       "Length: 2102, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "incorporated-partition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    Y1    Z1    X2    Y2    Z2  \n",
       "X1_4  Y1_4  Z1_5  X2_1  Y2_6  Z2_6    6\n",
       "X1_3  Y1_5  Z1_3  X2_3  Y2_6  Z2_2    6\n",
       "X1_2  Y1_1  Z1_1  X2_2  Y2_7  Z2_4    6\n",
       "      Y1_4  Z1_5  X2_4  Y2_2  Z2_2    6\n",
       "      Y1_1  Z1_3  X2_5  Y2_5  Z2_1    6\n",
       "                                     ..\n",
       "X1_4  Y1_4  Z1_7  X2_0  Y2_4  Z2_7    1\n",
       "      Y1_1  Z1_5  X2_2  Y2_2  Z2_0    1\n",
       "                        Y2_1  Z2_3    1\n",
       "X1_5  Y1_1  Z1_5  X2_4  Y2_3  Z2_2    1\n",
       "            Z1_4  X2_7  Y2_1  Z2_5    1\n",
       "Length: 2049, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "parliamentary-munich",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    Y1    Z1    X2    Y2    Z2  \n",
       "X1_0  Y1_2  Z1_2  X2_0  Y2_0  Z2_0    1256\n",
       "            Z1_5  X2_0  Y2_0  Z2_0    1077\n",
       "X1_6  Y1_0  Z1_2  X2_0  Y2_0  Z2_0     512\n",
       "X1_0  Y1_2  Z1_4  X2_0  Y2_0  Z2_0     415\n",
       "      Y1_4  Z1_4  X2_0  Y2_0  Z2_0     296\n",
       "                                      ... \n",
       "                  X2_4  Y2_0  Z2_7       1\n",
       "      Y1_0  Z1_2  X2_7  Y2_6  Z2_7       1\n",
       "      Y1_4  Z1_5  X2_1  Y2_7  Z2_7       1\n",
       "                  X2_7  Y2_0  Z2_7       1\n",
       "      Y1_2  Z1_7  X2_0  Y2_5  Z2_0       1\n",
       "Length: 125, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_4.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "every-granny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    Y1    Z1    X2    Y2    Z2  \n",
       "X1_0  Y1_4  Z1_5  X2_0  Y2_0  Z2_0    2089\n",
       "            Z1_4  X2_0  Y2_0  Z2_0     698\n",
       "      Y1_5  Z1_4  X2_0  Y2_0  Z2_0     639\n",
       "            Z1_5  X2_0  Y2_0  Z2_0     525\n",
       "      Y1_2  Z1_5  X2_0  Y2_0  Z2_0     355\n",
       "                                      ... \n",
       "      Y1_5  Z1_4  X2_0  Y2_0  Z2_7       1\n",
       "                  X2_1  Y2_0  Z2_1       1\n",
       "                        Y2_6  Z2_0       1\n",
       "      Y1_2  Z1_5  X2_3  Y2_7  Z2_0       1\n",
       "            Z1_7  X2_0  Y2_7  Z2_7       1\n",
       "Length: 115, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_5.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "designing-whale",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1    Y1    Z1    X2    Y2    Z2  \n",
       "X1_1  Y1_0  Z1_0  X2_0  Y2_0  Z2_0    1881\n",
       "      Y1_3  Z1_0  X2_0  Y2_0  Z2_0    1477\n",
       "            Z1_2  X2_0  Y2_0  Z2_0     782\n",
       "            Z1_5  X2_0  Y2_0  Z2_0     295\n",
       "            Z1_3  X2_0  Y2_0  Z2_0     160\n",
       "                                      ... \n",
       "      Y1_0  Z1_0  X2_6  Y2_6  Z2_1       1\n",
       "      Y1_3  Z1_7  X2_1  Y2_0  Z2_1       1\n",
       "            Z1_5  X2_7  Y2_3  Z2_7       1\n",
       "            Z1_2  X2_1  Y2_3  Z2_1       1\n",
       "            Z1_3  X2_1  Y2_0  Z2_5       1\n",
       "Length: 139, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_6.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "superb-columbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1_5    1288\n",
       "X1_0    1056\n",
       "X1_3     773\n",
       "X1_7     700\n",
       "X1_2     575\n",
       "X1_4     498\n",
       "X1_6      14\n",
       "Name: X1, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_1['X1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "medium-immune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y1_1    1944\n",
       "Y1_4    1779\n",
       "Y1_5     526\n",
       "Y1_6     463\n",
       "Y1_2     109\n",
       "Y1_7      83\n",
       "Name: Y1, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_1['Y1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "confident-recording",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Z1_5    1460\n",
       "Z1_4    1375\n",
       "Z1_1     932\n",
       "Z1_6     472\n",
       "Z1_7     323\n",
       "Z1_2     176\n",
       "Z1_3     166\n",
       "Name: Z1, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf_1['Z1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "mysterious-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensory_words_testdf_1 = sensory_words_testdf.loc[(sensory_words_testdf['activityID'] == 1)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_testdf_2 = sensory_words_testdf.loc[(sensory_words_testdf['activityID'] == 2)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_testdf_3 = sensory_words_testdf.loc[(sensory_words_testdf['activityID'] == 3)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_testdf_4 = sensory_words_testdf.loc[(sensory_words_testdf['activityID'] == 4)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_testdf_5 = sensory_words_testdf.loc[(sensory_words_testdf['activityID'] == 5)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]\n",
    "sensory_words_testdf_6 = sensory_words_testdf.loc[(sensory_words_testdf['activityID'] == 6)][['X1', 'Y1', 'Z1', 'X2', 'Y2', 'Z2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eligible-librarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1_0    1664\n",
       "X1_6     279\n",
       "X1_5      21\n",
       "Name: X1, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_testdf_4['X1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "settled-divorce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1_0    2128\n",
       "Name: X1, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_testdf_5['X1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bearing-synthesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1_1    2148\n",
       "Name: X1, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_testdf_6['X1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "modern-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Z1_5    5377\n",
       "Z1_0    3958\n",
       "Z1_2    3450\n",
       "Z2_2    2879\n",
       "Z1_4    2804\n",
       "Z2_3    1845\n",
       "Z2_1    1739\n",
       "Z2_5    1711\n",
       "Z2_6    1435\n",
       "Z2_4    1365\n",
       "Z2_7    1322\n",
       "Z1_3     752\n",
       "Z1_1     417\n",
       "Z1_7     263\n",
       "Z1_6      91\n",
       "Name: Z2, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_words_traindf['Z2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cathedral-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(sensory_words_traindf['Y2'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_0 -> _, Y2_0 -> _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "corrected-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "alien-repository",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y2_0 -- Z2_0\n"
     ]
    }
   ],
   "source": [
    "Y2_least_idf = collections.Counter(sensory_words_traindf['Y2'].tolist()).most_common()[0][0]\n",
    "Z2_least_idf = collections.Counter(sensory_words_traindf['Z2'].tolist()).most_common()[0][0]\n",
    "\n",
    "print(f'{Y2_least_idf} -- {Z2_least_idf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "incident-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement_word(channel_values, replaceword):\n",
    "\n",
    "    if channel_values[1] == replaceword:\n",
    "        return channel_values[0]\n",
    "    return channel_values[1]\n",
    "\n",
    "\n",
    "def replace_leastidf_values():\n",
    "\n",
    "    Y2_least_idf = collections.Counter(\n",
    "        sensory_words_traindf['Y2'].tolist()).most_common()[0][0]\n",
    "    Z2_least_idf = collections.Counter(\n",
    "        sensory_words_traindf['Z2'].tolist()).most_common()[0][0]\n",
    "\n",
    "    sensory_words_traindf['Y2'] = sensory_words_traindf[['Y1', 'Y2']].apply(\n",
    "        lambda row: get_replacement_word(row.values, Y2_least_idf), axis=1)\n",
    "\n",
    "    sensory_words_traindf['Z2'] = sensory_words_traindf[['Z1', 'Z2']].apply(\n",
    "        lambda row: get_replacement_word(row.values, Z2_least_idf), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "japanese-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_leastidf_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-freight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
